{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "353240d7-c4b1-4986-978b-ca654210f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac225a0-c0af-4584-a43e-2c92af4194d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "\n",
    "        # Add the padding token\n",
    "        self.__add_to_dict('<pad>')\n",
    "\n",
    "        # Add characters and numbers to the dictionary\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "\n",
    "        # Add space and punctuation to the dictionary\n",
    "        self.__add_to_dict('.')\n",
    "        self.__add_to_dict(' ')\n",
    "\n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            self.dictionary[character] = len(self.dictionary)\n",
    "            self.reverse_dictionary[self.dictionary[character]] = character\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.dictionary[c] for c in text]\n",
    "\n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary[character]\n",
    "\n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary[token]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a21fd7-8fcc-47be-8ae3-dca6dda68b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = '. '.join([\n",
    "    'cats rule the world',\n",
    "    'dogs are the best',\n",
    "    'elephants have long trunks',\n",
    "    'monkeys like bananas',\n",
    "    'pandas eat bamboo',\n",
    "    'tigers are dangerous',\n",
    "    'zebras have stripes',\n",
    "    'lions are the kings of the savannah',\n",
    "    'giraffes have long necks',\n",
    "    'hippos are big and scary',\n",
    "    'rhinos have horns',\n",
    "    'penguins live in the arctic',\n",
    "    'polar bears are white'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7822d77d-fcd5-47a9-81f4-176c1cdd5771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cats rule the world. dogs are the best. elephants have long trunks. monkeys like bananas. pandas eat bamboo. tigers are dangerous. zebras have stripes. lions are the kings of the savannah. giraffes have long necks. hippos are big and scary. rhinos have horns. penguins live in the arctic. polar bears are white'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87dc4c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38, 11, 28, 15, 38, 33, 18, 19, 30, 15]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_indices = Tokenizer().tokenize(training_data)\n",
    "token_indices[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eee8722-f031-4563-b3e6-b7912ebf5a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 11, 30, 29, 38],\n",
       " [11, 30, 29, 38, 28],\n",
       " [30, 29, 38, 28, 31],\n",
       " [29, 38, 28, 31, 22],\n",
       " [38, 28, 31, 22, 15],\n",
       " [28, 31, 22, 15, 38],\n",
       " [31, 22, 15, 38, 30],\n",
       " [22, 15, 38, 30, 18],\n",
       " [15, 38, 30, 18, 15],\n",
       " [38, 30, 18, 15, 38]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 5\n",
    "def create_seq(token_indices: list[int]):\n",
    "    sequences=[]\n",
    "    for i in range(0, len(token_indices) - MAX_SEQ_LEN+1):\n",
    "        sequences.append(token_indices[i:i+MAX_SEQ_LEN])\n",
    "    return sequences\n",
    "train_seq = create_seq(token_indices)  \n",
    "train_seq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b14b63-c83b-4d1e-8f31-4db1a16f6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X = torch.tensor(train_seq)[:,:-1]\n",
    "#Y = torch.tensor(train_seq)[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3552c1b0-11ea-4442-bdff-87f9fcc6ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BATCH_SIZE = 2\n",
    "def batchify(token_seq):\n",
    "    for i in range(0, len(token_seq)-MIN_BATCH_SIZE+1, MIN_BATCH_SIZE):\n",
    "        batch_as_list = token_seq[i:i+MIN_BATCH_SIZE]\n",
    "        X=torch.LongTensor(batch_as_list)[:,:-1] # last one for target\n",
    "        Y=torch.LongTensor(batch_as_list)[:,-1]\n",
    "        yield i,X,Y\n",
    "\n",
    "#for i,X,Y in batchify(train_seq):\n",
    "    #print(i,\"---\")\n",
    "    #display(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7009a3d-1ea7-4da1-9664-cfdee3d87929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vocab_size = Tokenizer().size()\n",
    "# X_hot = F.one_hot(X, vocab_size).sum(axis=1).type(torch.FloatTensor)\n",
    "# Y_hot = F.one_hot(Y, vocab_size).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1075454d-5805-453b-bc15-4fb5c6ecccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vocab_size = Tokenizer().size()\n",
    "NUM_HIDDEN = 10\n",
    "\n",
    "# my embedding\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        #self.W = torch.zeros(vocab_size, embedding_dim)\n",
    "        self.W = nn.parameter.Parameter(torch.zeros(vocab_size, embedding_dim))\n",
    "        \n",
    "        \n",
    "    def forward(self, X): # X is a batch\n",
    "        return self.W[X].sum(axis=1)\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, embedding_dim, padding_idx=0):\n",
    "        super().__init__()\n",
    "        #self.embedding = nn.Linear(vocab_size, embedding_dim)\n",
    "        #self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx,  max_norm=1)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "       \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        em = self.embedding(X)\n",
    "        \n",
    "        return self.output(em.sum(axis=1))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e66397c-43b3-44a2-b964-f6ff36f93eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.empty(3, dtype=torch.long).random_(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "432e5b1a-85f4-4608-837c-a01cf3853408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = Embedding(3,5)\n",
    "# em(batch)\n",
    "# batch = torch.LongTensor([[0,1],[0,2]])\n",
    "# W[batch].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13158e60-2e74-4415-ac75-0788947ca4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6205b4b-6863-463c-a1f0-0f7533ee23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(NUM_HIDDEN, Tokenizer().character_to_token(\"<pad>\"))\n",
    "#w2v_model(X_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4d51098-e6ad-4187-943e-bbf0ea1224a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model.embedding(torch.tensor([[1,2,3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb0e9662-e02c-4412-9f48-abd3a62fb399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.3688549995422363\n",
      "Epoch [2/100], Loss: 3.1380667686462402\n",
      "Epoch [3/100], Loss: 2.9523777961730957\n",
      "Epoch [4/100], Loss: 2.8204145431518555\n",
      "Epoch [5/100], Loss: 2.724544048309326\n",
      "Epoch [6/100], Loss: 2.6485695838928223\n",
      "Epoch [7/100], Loss: 2.5839121341705322\n",
      "Epoch [8/100], Loss: 2.5264928340911865\n",
      "Epoch [9/100], Loss: 2.4742612838745117\n",
      "Epoch [10/100], Loss: 2.4260783195495605\n",
      "Epoch [11/100], Loss: 2.381256103515625\n",
      "Epoch [12/100], Loss: 2.3393702507019043\n",
      "Epoch [13/100], Loss: 2.300474166870117\n",
      "Epoch [14/100], Loss: 2.2640490531921387\n",
      "Epoch [15/100], Loss: 2.229979991912842\n",
      "Epoch [16/100], Loss: 2.1981897354125977\n",
      "Epoch [17/100], Loss: 2.169308662414551\n",
      "Epoch [18/100], Loss: 2.147266387939453\n",
      "Epoch [19/100], Loss: 2.12632155418396\n",
      "Epoch [20/100], Loss: 2.106503963470459\n",
      "Epoch [21/100], Loss: 2.08780837059021\n",
      "Epoch [22/100], Loss: 2.0702104568481445\n",
      "Epoch [23/100], Loss: 2.0536792278289795\n",
      "Epoch [24/100], Loss: 2.0381767749786377\n",
      "Epoch [25/100], Loss: 2.0236663818359375\n",
      "Epoch [26/100], Loss: 2.0101096630096436\n",
      "Epoch [27/100], Loss: 1.9974660873413086\n",
      "Epoch [28/100], Loss: 1.985694169998169\n",
      "Epoch [29/100], Loss: 1.9747519493103027\n",
      "Epoch [30/100], Loss: 1.9645975828170776\n",
      "Epoch [31/100], Loss: 1.9551897048950195\n",
      "Epoch [32/100], Loss: 1.946486473083496\n",
      "Epoch [33/100], Loss: 1.9384467601776123\n",
      "Epoch [34/100], Loss: 1.9310290813446045\n",
      "Epoch [35/100], Loss: 1.9241951704025269\n",
      "Epoch [36/100], Loss: 1.9179069995880127\n",
      "Epoch [37/100], Loss: 1.912128210067749\n",
      "Epoch [38/100], Loss: 1.9068236351013184\n",
      "Epoch [39/100], Loss: 1.9019601345062256\n",
      "Epoch [40/100], Loss: 1.8975050449371338\n",
      "Epoch [41/100], Loss: 1.8934271335601807\n",
      "Epoch [42/100], Loss: 1.8896980285644531\n",
      "Epoch [43/100], Loss: 1.8862888813018799\n",
      "Epoch [44/100], Loss: 1.8831729888916016\n",
      "Epoch [45/100], Loss: 1.880324363708496\n",
      "Epoch [46/100], Loss: 1.8777191638946533\n",
      "Epoch [47/100], Loss: 1.8753321170806885\n",
      "Epoch [48/100], Loss: 1.873140573501587\n",
      "Epoch [49/100], Loss: 1.8711216449737549\n",
      "Epoch [50/100], Loss: 1.8692550659179688\n",
      "Epoch [51/100], Loss: 1.8675187826156616\n",
      "Epoch [52/100], Loss: 1.8658943176269531\n",
      "Epoch [53/100], Loss: 1.8643615245819092\n",
      "Epoch [54/100], Loss: 1.8629024028778076\n",
      "Epoch [55/100], Loss: 1.8614990711212158\n",
      "Epoch [56/100], Loss: 1.860135793685913\n",
      "Epoch [57/100], Loss: 1.858795404434204\n",
      "Epoch [58/100], Loss: 1.857462763786316\n",
      "Epoch [59/100], Loss: 1.8561230897903442\n",
      "Epoch [60/100], Loss: 1.8547616004943848\n",
      "Epoch [61/100], Loss: 1.8533661365509033\n",
      "Epoch [62/100], Loss: 1.8519227504730225\n",
      "Epoch [63/100], Loss: 1.85042142868042\n",
      "Epoch [64/100], Loss: 1.8488514423370361\n",
      "Epoch [65/100], Loss: 1.8472028970718384\n",
      "Epoch [66/100], Loss: 1.845468521118164\n",
      "Epoch [67/100], Loss: 1.8436412811279297\n",
      "Epoch [68/100], Loss: 1.8417143821716309\n",
      "Epoch [69/100], Loss: 1.839682698249817\n",
      "Epoch [70/100], Loss: 1.8375427722930908\n",
      "Epoch [71/100], Loss: 1.8352925777435303\n",
      "Epoch [72/100], Loss: 1.8329298496246338\n",
      "Epoch [73/100], Loss: 1.830456256866455\n",
      "Epoch [74/100], Loss: 1.8278717994689941\n",
      "Epoch [75/100], Loss: 1.8251773118972778\n",
      "Epoch [76/100], Loss: 1.8223751783370972\n",
      "Epoch [77/100], Loss: 1.8194694519042969\n",
      "Epoch [78/100], Loss: 1.8164639472961426\n",
      "Epoch [79/100], Loss: 1.8133585453033447\n",
      "Epoch [80/100], Loss: 1.8101963996887207\n",
      "Epoch [81/100], Loss: 1.8069555759429932\n",
      "Epoch [82/100], Loss: 1.8036283254623413\n",
      "Epoch [83/100], Loss: 1.8002228736877441\n",
      "Epoch [84/100], Loss: 1.7967476844787598\n",
      "Epoch [85/100], Loss: 1.7932093143463135\n",
      "Epoch [86/100], Loss: 1.7896157503128052\n",
      "Epoch [87/100], Loss: 1.7859737873077393\n",
      "Epoch [88/100], Loss: 1.7822906970977783\n",
      "Epoch [89/100], Loss: 1.7785720825195312\n",
      "Epoch [90/100], Loss: 1.774825930595398\n",
      "Epoch [91/100], Loss: 1.7710578441619873\n",
      "Epoch [92/100], Loss: 1.7672754526138306\n",
      "Epoch [93/100], Loss: 1.7634836435317993\n",
      "Epoch [94/100], Loss: 1.7596888542175293\n",
      "Epoch [95/100], Loss: 1.7558950185775757\n",
      "Epoch [96/100], Loss: 1.752109169960022\n",
      "Epoch [97/100], Loss: 1.748368740081787\n",
      "Epoch [98/100], Loss: 1.7446608543395996\n",
      "Epoch [99/100], Loss: 1.7409758567810059\n",
      "Epoch [100/100], Loss: 1.737316370010376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(w2v_model.parameters(), lr=1e-3) \n",
    "\n",
    "def train():\n",
    "    num_epochs=100\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,X,Y in batchify(train_seq):\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "            y_hat = w2v_model(X)\n",
    "            \n",
    "            #Y_vec=F.one_hot(Y,vocab_size).type(torch.LongTensor)\n",
    "            #print(X.shape,y_hat.shape)\n",
    "            loss = loss_fn(y_hat, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            #break\n",
    "        #display(w2v_model.embedding.W)\n",
    "        #if epoch ==0 or epoch == 99:\n",
    "        #    for p in w2v_model.embedding.parameters():\n",
    "        #        display(p)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c056a4f9-4219-422d-8b76-6008d5f52f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# def yield_tokens(file_path):\n",
    "#     with io.open(file_path, encoding = 'utf-8') as f:\n",
    "#         for line in f:\n",
    "#             yield line.strip().split()\n",
    "# train_file=\"/Users/mak/Downloads/wikitext-2/wiki.train.tokens\"\n",
    "# vocab = build_vocab_from_iterator(yield_tokens(train_file), specials=[\"<unk>\"])\n",
    "# vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad0ca2d3-d170-4748-89da-1849f70ceb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab.lookup_tokens(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720354d7-6618-4103-b16c-99a58f460343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
